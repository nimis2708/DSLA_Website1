id,title,section,level,overview,tags
ko-1,Azure Machine Learning Intermediate,"Professional Skills,Intermediate",Beginner,Learn Azure Machine Learning Intermediate in data science,"Azure Machine Learning, Overview: Azure Machine Learning is a cloud-based service provided by Microsoft that enables data scientists and machine learning engineers to build, train, deploy, and manage machine learning models at scale. With tools for automating model development and a broad array of pre-built algorithms, Azure ML simplifies the process of creating and deploying models. It integrates with other Azure services to provide a comprehensive machine learning pipeline, making it a powerful tool for enterprises and organizations working with large datasets., Learning Objectives: By the end of this topic, learners will be able to:, Understand the architecture of Azure Machine Learning and its key components., Build and train machine learning models using Azure ML Studio and SDK."
ko-2,Data Types And Data Structures Image Data Definition And Use Cases,"Professional Skills,Intermediate",Beginner,Learn Data Types And Data Structures Image Data Definition And Use Cases in data science,"Section:Data Types and Data Structures, Level: Intermediate, Overview, Image data refers to visual information captured or stored in a digital
format, represented as an array of pixels. Each pixel carries
information about intensity or color, depending on the type of image.
Image data is used extensively in fields such as computer vision,
graphics, and machine learning. Understanding the representation,
processing techniques, and applications of image data provides a
foundation for building solutions in object detection, medical imaging,
augmented reality, and more., Learning Objectives"
ko-3,Model Validation   Stratified Crossvalidation (1),"Professional Skills,Intermediate",Beginner,Learn Model Validation   Stratified Crossvalidation (1) in data science,"Level: Intermediate, Leave-One-Out Cross-Validation (LOO-CV) is a specialized form of K-Fold
Cross-Validation where the number of folds equals the number of data
points in the dataset. In this method, a single data point is used as
the validation set, and the remaining points are used for training. This
process is repeated for each data point, ensuring that every sample is
used for both training and validation. LOO-CV provides an exhaustive
evaluation of model performance, making it a highly robust validation
technique, especially for small datasets. However, its computational
cost can be prohibitive for large datasets due to the need for repeated
training., By the end of this module, you will be able to:, Define Leave-One-Out Cross-Validation and explain its significance
    in model validation., Define Leave-One-Out Cross-Validation and explain its significance
    in model validation."
ko-4,Aws Security  Best Practices For Data Science Projects (1),"Professional Skills,Intermediate",Beginner,Learn Aws Security  Best Practices For Data Science Projects (1) in data science,"AWS Security: Best Practices for Data Science Projects, Overview, AWS Security is crucial in data science projects to protect sensitive data, maintain compliance, and safeguard intellectual property. This knowledge object addresses AWS security services and best practices specifically for data science, covering topics like data encryption, secure access control, and compliance measures. Ensuring robust security practices minimizes risks and builds trust in cloud-based data science solutions., Learning Objectives, Understand AWS security fundamentals, focusing on data protection and secure infrastructure."
ko-5,"Data Types And Data Structures Data Structures Lists, Dictionaries,Tuples","Professional Skills,Intermediate",Beginner,"Learn Data Types And Data Structures Data Structures Lists, Dictionaries,Tuples in data science","Section:Data Types and Data Structures, Data Structures: Lists, Dictionaries, Tuples, Level: Beginner, Data structures are essential in programming for organizing, storing,
and manipulating data efficiently. Python provides versatile built-in
data structures, including lists, dictionaries, and tuples, which cater
to different use cases. Lists are ordered and mutable, dictionaries
store data as key-value pairs, and tuples are immutable sequences.
Understanding these structures and their operations enables developers
to write clean, efficient, and scalable code., By the end of this module, you will be able to:"
ko-6,"Using Row Number, Rank, And Dense Rank","Professional Skills,Intermediate",Beginner,"Learn Using Row Number, Rank, And Dense Rank in data science","Using ROW_NUMBER, RANK, and DENSE_RANK, Overview, ROW_NUMBER, RANK, and DENSE_RANK are powerful SQL window functions used to assign rankings to rows within a result set. They are commonly applied in analytical queries, such as creating sequential row numbers, ranking data, and handling ties in ordered datasets. These functions provide essential tools for organizing, analyzing, and filtering data without aggregating it., Learning Objectives, By the end of this topic, learners will be able to:"
ko-7,Model Evaluation And Deployment Data Handling Data Augmentation For Imbal,"Professional Skills,Intermediate",Beginner,Learn Model Evaluation And Deployment Data Handling Data Augmentation For Imbal in data science,"Data Handling: Data Augmentation for Imbalanced Datasets, Level: Intermediate, Data augmentation is a technique used to artificially increase the size
and diversity of a dataset by generating new data samples from existing
ones. For imbalanced datasets, data augmentation helps address the issue
of underrepresented classes by creating synthetic samples, thereby
improving model training and reducing bias toward the majority class.
Techniques such as random oversampling, SMOTE, and advanced augmentation
strategies tailored for images, text, or time series data are commonly
used in this context., By the end of this module, you will be able to:, Understand the role of data augmentation in handling imbalanced
    datasets."
ko-8,Database Design,"Professional Skills,Intermediate",Beginner,Learn Database Design in data science,"Database Design, Overview: Database design refers to the process of structuring a database to efficiently store, manage, and retrieve data. A well-designed database ensures data integrity, reduces redundancy, and improves the overall performance of database-driven applications. This topic covers key principles like normalization, relationships between tables, and best practices for ensuring scalability and efficiency in database management systems (DBMS). As businesses increasingly rely on data, understanding database design is critical for both developers and data engineers., Learning Objectives: By the end of this topic, learners will be able to:, Understand key concepts like data modeling, relationships, and normalization., Design efficient and scalable databases using relational database management systems (RDBMS)."
ko-9,Understanding Polymorphism In Python,"Professional Skills,Intermediate",Beginner,Learn Understanding Polymorphism In Python in data science,"Title: Understanding Polymorphism in PythonOverview:Polymorphism in Python refers to the ability of different objects to respond to the same method or function call in ways specific to their individual types. This concept is fundamental to object-oriented programming, allowing for more flexible and reusable code. Polymorphism enhances code readability, reduces complexity, and promotes the principle of ""write once, use many times"" in software development.Learning Objectives:, Understand the concept of polymorphism and its importance in Python programming, Identify and implement different types of polymorphism: function polymorphism, class polymorphism, and operator polymorphism, Apply polymorphic principles to create more flexible and maintainable code, Recognize how polymorphism interacts with inheritance and method overriding"
ko-10,Using Sql With Time Series Data,"Professional Skills,Intermediate",Beginner,Learn Using Sql With Time Series Data in data science,"Time series data refers to data points collected or recorded at specific time intervals. Analyzing time series data is essential for understanding trends, patterns, and seasonal variations in various fields such as finance, healthcare, and marketing. SQL offers several features that allow analysts to efficiently query, manipulate, and aggregate time series data for forecasting, anomaly detection, and trend analysis. This topic explores how to work with time series data in SQL and apply it to solve real-world problems., By the end of this module, learners will be able to:, ●   Understand the fundamentals of time series data and its unique challenges.●   Use SQL to extract, filter, and transform time-based data for analysis.●   Implement common time series analysis techniques such as rolling averages, period comparisons, and seasonal decomposition using SQL.●   Work with different SQL functions to perform date and time-based operations (e.g., DATE_TRUNC, EXTRACT, and DATE_ADD).●   Handle missing or irregular time series data using SQL techniques.●   Apply SQL to perform trend analysis, forecasting, and anomaly detection on time series data., ●Basic SQL Knowledge: Familiarity with SQL syntax such as SELECT, WHERE, GROUP BY, and JOIN.●Intermediate SQL Skills: Understanding of aggregation functions (e.g., SUM, AVG) and the ability to filter data using conditions.●Date and Time Functions: Basic knowledge of date and time functions in SQL, such as DATE, DATETIME, and TIMESTAMP.●Data Analysis Concepts: Understanding of time series data and its characteristics, such as trends, seasonality, and noise.●Tools: Access to a SQL database management system or SQL environment like MySQL, PostgreSQL, or SQL Server., ●What is Time Series Data?Time series data consists of data points that are indexed by time. It is typically collected at regular intervals (e.g., daily, monthly, or hourly) and represents trends or patterns over time.●Basic Date and Time Functions○DATE_TRUNC: Truncates a timestamp to a specified date part (e.g., day, month, year).○EXTRACT: Extracts parts of a date or timestamp, such as year, month, day, etc.○DATE_ADD: Adds a specified interval (e.g., days, months) to a date.○Example: Aggregating sales data by month, using the EXTRACT function to pull out the month part of a timestamp.●Aggregating Time Series Data○   You can aggregate time series data by specific time periods such as year, month, or week. For example, you can calculate the total sales per month or the average temperature per year."
ko-11,"Handling Nulls In Sql  Coalesce, Ifnull","Professional Skills,Intermediate",Beginner,"Learn Handling Nulls In Sql  Coalesce, Ifnull in data science","Handling NULLs in SQL: COALESCE, IFNULL, Overview, Handling NULL values in SQL is critical for ensuring data integrity and consistency in queries. Functions like COALESCE and IFNULL help manage NULL values by providing default values or alternative expressions, avoiding potential errors or misinterpretations in results. These tools are vital for building robust SQL queries that handle incomplete or missing data effectively., Learning Objectives, By the end of this topic, learners will be able to:"
ko-12,Knowledge Object  Sql Joins (Intermediate Level),"Professional Skills,Intermediate",Beginner,Learn Knowledge Object  Sql Joins (Intermediate Level) in data science,"Knowledge Object: SQL JOINS (Intermediate Level), Title:, SQL JOINS for Intermediate Learners, Overview:, SQL JOINS are a fundamental concept in relational databases, allowing users to combine records from two or more tables based on a related column. As the complexity of database systems grows, understanding how to efficiently execute JOINS is critical for data retrieval and analysis. Intermediate learners will explore deeper concepts such as different JOIN types, optimization strategies, and use cases."
ko-13,Aggregation In Sql,"Professional Skills,Intermediate",Beginner,Learn Aggregation In Sql in data science,"Aggregation in SQL, Overview: Aggregation in SQL refers to the process of summarizing large datasets by performing calculations such as sums, averages, counts, and more. It is essential for data analysis, reporting, and business intelligence, allowing users to derive insights from raw data. With growing data complexity, understanding advanced aggregation techniques is critical for handling modern database systems efficiently., Learning Objectives: By the end of this topic, learners will be able to:, Understand fundamental aggregation operations like SUM(), AVG(), COUNT(), MIN(), MAX()., Implement aggregation with GROUP BY and HAVING clauses in SQL queries."
ko-14,Data Types And Data Structures Handling Complex Data Types (Geospatial,"Professional Skills,Intermediate",Beginner,Learn Data Types And Data Structures Handling Complex Data Types (Geospatial in data science,"Section:Data Types and Data Structures, Handling Complex Data Types (Geospatial, Audio, Video), Level: Intermediate, Complex data types such as geospatial, audio, and video data require
specialized techniques for processing, analyzing, and extracting
meaningful insights. Geospatial data includes geographic coordinates and
maps, audio data involves sound waveforms, and video data is a sequence
of image frames. Handling these data types is crucial for tasks in areas
like location-based services, speech recognition, and video analytics., By the end of this module, you will be able to:"
ko-15,Sql Window Functions,"Professional Skills,Intermediate",Beginner,Learn Sql Window Functions in data science,"SQL Window Functions, Overview, SQL Window Functions are powerful tools used for performing calculations across a set of table rows that are related to the current row. Unlike aggregate functions, which summarize data into a single value, window functions allow you to retain the individual rows while applying calculations like rankings, averages, or running totals. These functions are particularly useful in analytics and reporting tasks, enabling complex calculations over partitions of data while maintaining row-level details., Learning Objectives, By the end of this topic, learners will be able to:"
ko-16,Model Evaluation And Deployment Data Handling   Handling Imbalanced Datasets,"Professional Skills,Intermediate",Beginner,Learn Model Evaluation And Deployment Data Handling   Handling Imbalanced Datasets in data science,"Model Evaluation and Deployment: Handling Imbalanced Datasets, Level-Intermediate, Imbalanced datasets are prevalent in real-world machine learning
problems, where one class significantly outnumbers others. Examples
include fraud detection, rare disease diagnosis, and anomaly detection.
When training models on imbalanced datasets, the algorithms often
prioritize the majority class, leading to biased models with poor
predictive performance for the minority class. Addressing this issue
involves understanding the nature of imbalance, employing specific
preprocessing techniques, using appropriate evaluation metrics, and
deploying robust models that can handle imbalance during inference., By the end of this module, you will be able to:, Define imbalanced datasets and understand the challenges they
    present."
